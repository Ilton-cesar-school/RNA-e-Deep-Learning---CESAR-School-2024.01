{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhAGl8wthEEs"
      },
      "source": [
        "## Projeto Final - Redes Neurais\n",
        "\n",
        "Ilton Albuquerque Martins de Lima - iaml@cesar.school"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sulShD5y4K7V"
      },
      "source": [
        "# Treinamento com interface de alto nível"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mALEfpx54K7d"
      },
      "source": [
        "## Importação das bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aOn4IXXnGRBg"
      },
      "outputs": [],
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WN69WSbu4K7f"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from PIL import UnidentifiedImageError\n",
        "from torch.utils.data import Dataset, random_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzAXEgxguXre"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72xfai7GHs1t",
        "outputId": "27660e18-f5aa-46bc-a2b5-7acb30e3a8ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 86.7M  100 86.7M    0     0  55.3M      0  0:00:01  0:00:01 --:--:--  128M\n"
          ]
        }
      ],
      "source": [
        "#Importação do Dataset\n",
        "!curl -L -o /content/archive.zip https://www.kaggle.com/api/v1/datasets/download/sartajbhuvaji/brain-tumor-classification-mri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HwLeGE7JH4jk"
      },
      "outputs": [],
      "source": [
        "#  Criar pasta do dataset\n",
        "!unzip -q -o /content/archive.zip -d /content/brain-tumor-classification-mri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YFIG3-USIAPH"
      },
      "outputs": [],
      "source": [
        "# Selecionando Pasta do Dataset\n",
        "brain_tumor_folder = \"/content/brain-tumor-classification-mri\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5Cez06ZGvSy6"
      },
      "outputs": [],
      "source": [
        "# Processo de Criação do Dataset\n",
        "transform=transforms.Compose([\n",
        "    transforms.Resize((64,64)),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fORDyj5Uvowm"
      },
      "outputs": [],
      "source": [
        "# Função para não trazer erros das imagens\n",
        "class CustomDataset(datasets.ImageFolder):\n",
        "  def __getitem__(self, index):\n",
        "    try:\n",
        "      return super(CustomDataset, self).__getitem__(index)\n",
        "    except UnidentifiedImageError:\n",
        "      print(f\"Skipped problematic image {index}\")\n",
        "      return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "R11Zs1qZxfvF"
      },
      "outputs": [],
      "source": [
        "# Criando Dataset\n",
        "dataset = CustomDataset(brain_tumor_folder, transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hTbh0IkFyIDM"
      },
      "outputs": [],
      "source": [
        "# Dividindo Dataset Treino e Teste\n",
        "dataset_train, dataset_test = random_split(dataset, [0.7, 0.3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOgzedZ_yZfG",
        "outputId": "fa3be02a-226b-4d1f-f2f0-3bd93d64dcf5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2285"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Tamanho do Dataset Treino\n",
        "len(dataset_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_5ej-ZcybLF",
        "outputId": "60152c25-bdc7-415a-9967-3bec6c26a670"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "979"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Tamanho do Dataset Test\n",
        "len(dataset_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp1z8PqOymws"
      },
      "source": [
        "## DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dAq1qqAIy0dT"
      },
      "outputs": [],
      "source": [
        "# Função para lidar com dados com erro\n",
        "def custom_collate_fn(batch):\n",
        "  batch = list(filter(lambda x: x is not None, batch))\n",
        "  return torch.utils.data.default_collate(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "mau9VWL2ypFz"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(1111)\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "train_kwargs = {'batch_size': 64}\n",
        "test_kwargs = {'batch_size': 100}\n",
        "if use_cuda:\n",
        "    cuda_kwargs = {'num_workers': 1,\n",
        "                    'pin_memory': True,\n",
        "                    'shuffle': True}\n",
        "    train_kwargs.update(cuda_kwargs)\n",
        "    test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset_train,**train_kwargs, collate_fn=custom_collate_fn)\n",
        "test_loader = torch.utils.data.DataLoader(dataset_test, **test_kwargs, collate_fn=custom_collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T8iS1Jc4K7q"
      },
      "source": [
        "## Criação da rede"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RUi00oew4K7q"
      },
      "outputs": [],
      "source": [
        "# Criação da Rede Neural com 64*64 de entrada e 3 de saida\n",
        "input_size = 64*64\n",
        "output_size = 4\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_size, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4096, 8192),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(8192, 8192),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(8192, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4096, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 4)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, input_size)\n",
        "        x = self.fc(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "model = Net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-HsB4r6mXdR",
        "outputId": "395bad70-0693-440c-909e-0d1767c9e5b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=4096, out_features=2048, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=2048, out_features=4096, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=4096, out_features=8192, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=8192, out_features=8192, bias=True)\n",
              "    (7): ReLU()\n",
              "    (8): Linear(in_features=8192, out_features=4096, bias=True)\n",
              "    (9): ReLU()\n",
              "    (10): Linear(in_features=4096, out_features=2048, bias=True)\n",
              "    (11): ReLU()\n",
              "    (12): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "    (13): ReLU()\n",
              "    (14): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (15): ReLU()\n",
              "    (16): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (17): ReLU()\n",
              "    (18): Linear(in_features=256, out_features=4, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Modelo\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRjwMbTd0JOC"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "400Lbat24K7v"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqLnH5gE0K9J",
        "outputId": "83af3bd3-acf3-4559-8702-0a13de8e1d4c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.3338, -1.3919, -1.4158, -1.4058]], grad_fn=<LogSoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "model(dataset_train[5][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cqllMDO4K7y"
      },
      "source": [
        "### Criando o objeto de treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OAEOQoZy4K72"
      },
      "outputs": [],
      "source": [
        "# Treinamento\n",
        "def train(log_interval, dry_run, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "            if dry_run:\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "eE6DjiKK4K76"
      },
      "outputs": [],
      "source": [
        "# Teste\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    acc = 100. * correct / len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        acc))\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxdm4FTK4K8E"
      },
      "source": [
        "## Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgH7e25qb_-K",
        "outputId": "8ed26127-88cc-4ce0-a23f-c2b58d4c7197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.3844, Accuracy: 127/979 (13%)\n",
            "\n",
            "Train Epoch: 1 [0/2285 (0%)]\tLoss: 1.383732\n",
            "Train Epoch: 1 [640/2285 (28%)]\tLoss: 0.439662\n",
            "Train Epoch: 1 [1280/2285 (56%)]\tLoss: 0.471639\n",
            "Train Epoch: 1 [1920/2285 (83%)]\tLoss: 0.253452\n",
            "\n",
            "Test set: Average loss: 0.5602, Accuracy: 852/979 (87%)\n",
            "\n",
            "Train Epoch: 2 [0/2285 (0%)]\tLoss: 0.567836\n",
            "Train Epoch: 2 [640/2285 (28%)]\tLoss: 0.350952\n",
            "Train Epoch: 2 [1280/2285 (56%)]\tLoss: 0.286835\n",
            "Train Epoch: 2 [1920/2285 (83%)]\tLoss: 0.291434\n",
            "\n",
            "Test set: Average loss: 0.3501, Accuracy: 852/979 (87%)\n",
            "\n",
            "Train Epoch: 3 [0/2285 (0%)]\tLoss: 0.332062\n",
            "Train Epoch: 3 [640/2285 (28%)]\tLoss: 0.357188\n",
            "Train Epoch: 3 [1280/2285 (56%)]\tLoss: 0.289600\n",
            "Train Epoch: 3 [1920/2285 (83%)]\tLoss: 0.249953\n",
            "\n",
            "Test set: Average loss: 0.3329, Accuracy: 852/979 (87%)\n",
            "\n",
            "Train Epoch: 4 [0/2285 (0%)]\tLoss: 0.300694\n",
            "Train Epoch: 4 [640/2285 (28%)]\tLoss: 0.388613\n",
            "Train Epoch: 4 [1280/2285 (56%)]\tLoss: 0.268674\n",
            "Train Epoch: 4 [1920/2285 (83%)]\tLoss: 0.275735\n",
            "\n",
            "Test set: Average loss: 0.3217, Accuracy: 852/979 (87%)\n",
            "\n",
            "Train Epoch: 5 [0/2285 (0%)]\tLoss: 0.281019\n",
            "Train Epoch: 5 [640/2285 (28%)]\tLoss: 0.334873\n",
            "Train Epoch: 5 [1280/2285 (56%)]\tLoss: 0.217001\n",
            "Train Epoch: 5 [1920/2285 (83%)]\tLoss: 0.255316\n",
            "\n",
            "Test set: Average loss: 0.3150, Accuracy: 852/979 (87%)\n",
            "\n",
            "Train Epoch: 6 [0/2285 (0%)]\tLoss: 0.257381\n",
            "Train Epoch: 6 [640/2285 (28%)]\tLoss: 0.322726\n",
            "Train Epoch: 6 [1280/2285 (56%)]\tLoss: 0.179543\n",
            "Train Epoch: 6 [1920/2285 (83%)]\tLoss: 0.226120\n",
            "\n",
            "Test set: Average loss: 0.3303, Accuracy: 852/979 (87%)\n",
            "\n",
            "Train Epoch: 7 [0/2285 (0%)]\tLoss: 0.265661\n",
            "Train Epoch: 7 [640/2285 (28%)]\tLoss: 0.320583\n",
            "Train Epoch: 7 [1280/2285 (56%)]\tLoss: 0.185472\n",
            "Train Epoch: 7 [1920/2285 (83%)]\tLoss: 0.244620\n",
            "\n",
            "Test set: Average loss: 0.3887, Accuracy: 815/979 (83%)\n",
            "\n",
            "Train Epoch: 8 [0/2285 (0%)]\tLoss: 0.291947\n",
            "Train Epoch: 8 [640/2285 (28%)]\tLoss: 0.318887\n",
            "Train Epoch: 8 [1280/2285 (56%)]\tLoss: 0.184158\n",
            "Train Epoch: 8 [1920/2285 (83%)]\tLoss: 0.267576\n",
            "\n",
            "Test set: Average loss: 0.3572, Accuracy: 852/979 (87%)\n",
            "\n",
            "Train Epoch: 9 [0/2285 (0%)]\tLoss: 0.305785\n",
            "Train Epoch: 9 [640/2285 (28%)]\tLoss: 0.322981\n",
            "Train Epoch: 9 [1280/2285 (56%)]\tLoss: 0.182606\n",
            "Train Epoch: 9 [1920/2285 (83%)]\tLoss: 0.225264\n",
            "\n",
            "Test set: Average loss: 0.3172, Accuracy: 852/979 (87%)\n",
            "\n",
            "Train Epoch: 10 [0/2285 (0%)]\tLoss: 0.224453\n",
            "Train Epoch: 10 [640/2285 (28%)]\tLoss: 0.315011\n",
            "Train Epoch: 10 [1280/2285 (56%)]\tLoss: 0.171556\n",
            "Train Epoch: 10 [1920/2285 (83%)]\tLoss: 0.208101\n",
            "\n",
            "Test set: Average loss: 0.3486, Accuracy: 848/979 (87%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = Net().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 10\n",
        "scheduler = StepLR(optimizer, step_size=4, gamma=0.7)\n",
        "best_acc = test(model, device, test_loader)\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(10, False, model, device, train_loader, optimizer, epoch)\n",
        "    acc = test(model, device, test_loader)\n",
        "    if acc > best_acc:\n",
        "      torch.save(model.state_dict(), \"brain_tumor_nn.pt\")\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kKUxLuJpDB9"
      },
      "outputs": [],
      "source": [
        "aux = torch.load(\"brain_tumor_nn.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gpjoGcjpJyn"
      },
      "outputs": [],
      "source": [
        "aux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3leKt2uEpL5H"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(aux)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Meu dataset escolhido foi o brain tumor onde as imagens são classificação de tumores cerebrais. Foi usado o tamanho 64*64 e a saída de 4. Usei uma rede um pouco complexa, por se tratar de imagens. Usei o batch_size de treinamento de 64 e teste de 100. E foi usada 10 épocas. A Acurácia deu um máximo de: 87% o que é um bom número e a menor perda foi de: 0.171556 na 10 época.  "
      ],
      "metadata": {
        "id": "TRq9kGgsu3fG"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "5fe3e6f0cdaab8afdc61c52912fda83f7c0a71baaea1897dd7498e2df01e69ec"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}